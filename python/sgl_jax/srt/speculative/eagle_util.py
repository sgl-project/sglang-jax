from __future__ import annotations

import copy
import logging
import os
from dataclasses import dataclass
from typing import Any

import jax
import jax.numpy as jnp
import numpy
from flax import nnx
from jax.tree_util import register_pytree_node_class

from sgl_jax.srt.layers.logits_processor import LogitsProcessorOutput
from sgl_jax.srt.managers.schedule_batch import (
    ScheduleBatch,
    get_last_loc,
    global_server_args_dict,
)
from sgl_jax.srt.mem_cache.allocator import BaseTokenToKVPoolAllocator
from sgl_jax.srt.model_executor.forward_batch_info import CaptureHiddenMode
from sgl_jax.srt.speculative.pallas.kernel import (
    align_evict_mask_to_page_size,
    assign_req_to_token_pool,
    create_extend_after_decode_spec_info,
    filter_finished_cache_loc_kernel,
    get_target_cache_loc,
    top_k_renorm_prob,
    top_p_renorm_prob,
    tree_speculative_sampling_target_only,
    verify_tree_greedy,
)

logger = logging.getLogger(__name__)

SIMULATE_ACC_LEN = os.environ.get("SIMULATE_ACC_LEN")
SIMULATE_ACC_METHOD = os.environ.get("SIMULATE_ACC_METHOD", "multinomial")


def _is_jax_leaf(value: Any) -> bool:
    """Detect sentinel nodes generated by jax.tree_util when shaping pytrees."""
    cls = value.__class__
    return cls.__name__ == "Leaf" and cls.__module__.startswith("jax.")


def _as_int32_array(value: Any, *, fallback: int = -1) -> jax.Array:
    """Convert scalar-like inputs into scalar int32 JAX arrays."""
    if isinstance(value, jax.Array):
        return value
    if isinstance(value, numpy.ndarray):
        return jnp.asarray(value, dtype=jnp.int32)
    if isinstance(value, (int, numpy.integer)):
        return jnp.asarray(int(value), dtype=jnp.int32)
    if isinstance(value, (list, tuple)):
        return jnp.asarray(value, dtype=jnp.int32)
    if _is_jax_leaf(value):
        return jnp.asarray(fallback, dtype=jnp.int32)
    try:
        return jnp.asarray(value, dtype=jnp.int32)
    except (TypeError, ValueError) as exc:
        raise TypeError(
            f"Unable to convert value of type {type(value)} into int32 metadata array."
        ) from exc


def get_last_loc_jax_array(
    req_to_token: jax.Array,
    req_pool_indices: jax.Array,
    prefix_lens: jax.Array,
) -> jax.Array:
    """JAX version of get_last_loc that operates on JAX arrays.

    Args:
        req_to_token: Token mapping tensor of shape (num_reqs, max_seq_len)
        req_pool_indices: Request pool indices of shape (batch_size,)
        prefix_lens: Prefix lengths of shape (batch_size,)

    Returns:
        Last location tensor of shape (batch_size,)
    """
    return jnp.where(
        prefix_lens > 0,
        req_to_token[req_pool_indices, prefix_lens - 1],
        jnp.full_like(prefix_lens, -1),
    )


def get_last_loc_large_page_size_top_k_1(
    req_to_token: jax.Array,
    req_pool_indices: jax.Array,
    seq_lens: jax.Array,
    speculative_num_steps: int,
) -> tuple[jax.Array, jax.Array, jax.Array]:
    """JAX implementation of get_last_loc_large_page_size_top_k_1.

    This function is used in EAGLE speculative decoding to compute cache locations
    for large page sizes when top_k=1.

    Args:
        req_to_token: Request to token mapping tensor
        req_pool_indices: Request pool indices
        seq_lens: Current sequence lengths
        speculative_num_steps: Number of speculative decoding steps

    Returns:
        tuple of (prefix_lens, new_seq_lens, last_loc):
        - prefix_lens: Same as input seq_lens
        - new_seq_lens: Updated sequence lengths (prefix_lens + speculative_num_steps)
        - last_loc: Last cache locations computed using get_last_loc
    """
    prefix_lens = seq_lens
    new_seq_lens = prefix_lens + speculative_num_steps
    last_loc = get_last_loc_jax_array(
        req_to_token,
        req_pool_indices,
        prefix_lens,
    )
    return prefix_lens, new_seq_lens, last_loc


def get_last_loc_large_page_size_large_top_k(
    req_to_token: jax.Array,
    req_pool_indices: jax.Array,
    seq_lens: jax.Array,
    speculative_num_steps: int,
    topk: int,
    page_size: int,
) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:
    """JAX implementation of get_last_loc_large_page_size_large_top_k.

    This function handles large page sizes with large top_k values in EAGLE speculative decoding.
    It computes cache locations and manages page allocation for multiple top-k branches.

    Args:
        req_to_token: Request to token mapping tensor
        req_pool_indices: Request pool indices
        seq_lens: Current sequence lengths
        speculative_num_steps: Number of speculative decoding steps
        topk: Number of top-k branches
        page_size: Size of each memory page

    Returns:
        tuple of (prefix_lens, new_seq_lens, last_loc, num_new_pages_per_topk, extend_lens):
        - prefix_lens: Same as input seq_lens
        - new_seq_lens: Updated sequence lengths considering page alignment
        - last_loc: Last cache locations
        - num_new_pages_per_topk: Number of new pages needed per top-k branch
        - extend_lens: Number of tokens to extend for each sequence
    """
    prefix_lens = seq_lens
    last_page_lens = prefix_lens % page_size
    num_new_pages_per_topk = (last_page_lens + speculative_num_steps + page_size - 1) // page_size

    new_seq_lens = prefix_lens // page_size * page_size + num_new_pages_per_topk * (
        page_size * topk
    )
    extend_lens = new_seq_lens - prefix_lens

    last_loc = get_last_loc_jax_array(
        req_to_token,
        req_pool_indices,
        prefix_lens,
    )

    return prefix_lens, new_seq_lens, last_loc, num_new_pages_per_topk, extend_lens


def build_tree_kernel_efficient_preprocess(
    verified_id: jax.Array,
    score_list: list[jax.Array],
    token_list: list[jax.Array],
    parents_list: list[jax.Array],
    num_verify_tokens: int,
):
    # Concatenate score_list along dim=1 and flatten from dim=1 onwards
    # b, n, topk; n = 1 + (num_steps-1) * self.topk
    score_tensor = jnp.concatenate(score_list, axis=1)
    score_tensor = score_tensor.reshape(score_tensor.shape[0], -1)

    # Concatenate token lists: b, (self.topk + (num_steps-1) * self.topk)
    ss_token_list = jnp.concatenate(token_list, axis=1)

    # Get top scores and indices
    _, top_scores_index = jax.lax.top_k(score_tensor, num_verify_tokens - 1)
    top_scores_index = jnp.sort(top_scores_index, axis=-1)

    # Gather draft tokens using the top indices
    draft_tokens = jnp.take_along_axis(ss_token_list, top_scores_index, axis=1)
    # assert draft_tokens.shape == (batch_size, verified_id.shape[0])
    draft_tokens = jnp.concatenate(
        [jnp.expand_dims(verified_id, axis=1), draft_tokens], axis=1
    ).flatten()

    # Build parent list
    if len(parents_list) > 1:
        parent_list = jnp.concatenate(parents_list[:-1], axis=1)
    else:
        batch_size = parents_list[0].shape[0]
        parent_list = jnp.empty((batch_size, 0), dtype=jnp.int32)

    return parent_list, top_scores_index, draft_tokens


def build_tree_kernel_efficient(
    verified_id: jax.Array,
    score_list: list[jax.Array],
    token_list: list[jax.Array],
    parents_list: list[jax.Array],
    seq_lens: jax.Array,
    seq_lens_sum: int,
    topk: int,
    spec_steps: int,
    num_verify_tokens: int,
    max_seq_len_per_req: int,
) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:
    """JAX implementation of build_tree_kernel_efficient.

    Args:
        verified_id: Verified token IDs from previous step
        score_list: List of score tensors from draft model
        token_list: List of token tensors from draft model
        parents_list: List of parent index tensors
        seq_lens: Sequence lengths
        seq_lens_sum: Sum of sequence lengths
        topk: Number of top-k candidates
        spec_steps: Number of speculative steps
        num_verify_tokens: Number of tokens to verify
        max_seq_len_per_req: Maximum allowed sequence length per request (static bound)

    Returns:
        tuple of (tree_mask, positions, retrive_index, retrive_next_token,
                 retrive_next_sibling, draft_tokens)
    """
    parent_list, top_scores_index, draft_tokens = build_tree_kernel_efficient_preprocess(
        verified_id, score_list, token_list, parents_list, num_verify_tokens
    )

    # Get batch size
    bs = seq_lens.shape[0]
    actual_tree_mask_size = (
        seq_lens_sum * num_verify_tokens + num_verify_tokens * num_verify_tokens * bs
    )
    max_tree_mask_size = (
        max_seq_len_per_req * num_verify_tokens * bs + num_verify_tokens * num_verify_tokens * bs
    )

    tree_mask, positions, retrive_index, retrive_next_token, retrive_next_sibling = (
        build_eagle_tree_structure(
            parent_list=parent_list,
            selected_index=top_scores_index,
            verified_seq_len=seq_lens,
            bs=bs,
            draft_token_num=num_verify_tokens,
            topk=topk,
            depth=spec_steps,
            seq_lens_sum=seq_lens_sum,
            tree_mask_mode=0,  # FULL_MASK
            max_seq_len_per_req=max_seq_len_per_req,
            max_tree_mask_size=max_tree_mask_size,
            actual_tree_mask_size=actual_tree_mask_size,
        )
    )

    return (
        tree_mask,
        positions,
        retrive_index,
        retrive_next_token,
        retrive_next_sibling,
        draft_tokens,
    )


@register_pytree_node_class
@dataclass
class EagleDraftInput:
    # The inputs for decode
    # shape: (b, topk)
    topk_p: jax.Array = None
    topk_index: jax.Array = None
    # shape: (b, hidden_size)
    hidden_states: jax.Array = None
    capture_hidden_mode: CaptureHiddenMode = CaptureHiddenMode.FULL

    # Inputs for extend
    # shape: (b,)
    verified_id: jax.Array = None
    accept_length: jax.Array = None
    accept_length_cpu: jax.Array | None = None

    # Inputs for the attention backends
    # shape: (b + 1,)
    kv_indptr: jax.Array = None
    kv_indices: jax.Array = None

    # Shape info for padding
    num_tokens_per_batch: int = -1
    num_tokens_for_logprob_per_batch: int = -1

    # Inputs for draft extend
    # shape: (b,)
    seq_lens_for_draft_extend: jax.Array = None
    req_pool_indices_for_draft_extend: jax.Array = None

    def tree_flatten(self):
        accept_length_cpu_arr = (
            jnp.empty((0,), dtype=jnp.int32)
            if self.accept_length_cpu is None
            else _as_int32_array(self.accept_length_cpu, fallback=0)
        )

        num_tokens_per_batch_arr = _as_int32_array(self.num_tokens_per_batch)
        num_tokens_for_logprob_arr = _as_int32_array(self.num_tokens_for_logprob_per_batch)

        children = (
            self.topk_p,
            self.topk_index,
            self.hidden_states,
            self.verified_id,
            self.accept_length,
            self.kv_indptr,
            self.kv_indices,
            self.seq_lens_for_draft_extend,
            self.req_pool_indices_for_draft_extend,
            accept_length_cpu_arr,
            num_tokens_per_batch_arr,
            num_tokens_for_logprob_arr,
        )

        aux_data = {
            "capture_hidden_mode": self.capture_hidden_mode,
        }
        return (children, aux_data)

    @classmethod
    def tree_unflatten(cls, aux_data, children):
        obj = cls.__new__(cls)
        obj.capture_hidden_mode = aux_data["capture_hidden_mode"]
        obj.topk_p = children[0]
        obj.topk_index = children[1]
        obj.hidden_states = children[2]
        obj.verified_id = children[3]
        obj.accept_length = children[4]
        obj.kv_indptr = children[5]
        obj.kv_indices = children[6]
        obj.seq_lens_for_draft_extend = children[7]
        obj.req_pool_indices_for_draft_extend = children[8]

        obj.accept_length_cpu = children[9]
        obj.num_tokens_per_batch = children[10]
        obj.num_tokens_for_logprob_per_batch = children[11]

        return obj

    def prepare_for_extend(self, batch: ScheduleBatch):

        if batch.forward_mode.is_idle():
            return

        # Prefill only generate 1 token.
        assert len(self.verified_id) == len(batch.seq_lens)

        pt = 0
        for i, extend_len in enumerate(batch.extend_lens):
            input_ids = batch.input_ids[pt : pt + extend_len]
            # TODO: batch.input_ids should on tpu
            batch.input_ids[pt : pt + extend_len] = jnp.concatenate(
                (input_ids[1:], self.verified_id[i].reshape(1))
            )
            pt += extend_len

    @classmethod
    def create_idle_input(
        cls,
        hidden_size: int,
        dtype: jnp.dtype,
        topk: int,
        capture_hidden_mode: CaptureHiddenMode,
    ):
        return cls(
            verified_id=jnp.empty((0,), dtype=jnp.int32),
            hidden_states=jnp.empty((0, hidden_size), dtype=dtype),
            topk_p=jnp.empty((0, topk), dtype=jnp.float32),
            topk_index=jnp.empty((0, topk), dtype=jnp.int32),
            capture_hidden_mode=capture_hidden_mode,
            accept_length=jnp.empty((0,), dtype=jnp.int32),
            accept_length_cpu=jnp.empty((0,), dtype=jnp.int32),
        )

    def prepare_extend_after_decode(
        self,
        batch: ScheduleBatch,
    ):
        if batch.forward_mode.is_idle():
            return

        batch.input_ids = self.verified_id
        accept_length_cpu_arr = batch.spec_info.accept_length_cpu
        if accept_length_cpu_arr is None:
            accept_length_cpu_host = numpy.asarray([], dtype=numpy.int32)
        else:
            accept_length_cpu_host = numpy.asarray(
                jax.device_get(accept_length_cpu_arr), dtype=numpy.int32
            )
        batch.extend_lens = (accept_length_cpu_host + 1).tolist()
        batch.extend_num_tokens = sum(batch.extend_lens)
        batch.seq_lens = batch.spec_info.seq_lens_for_draft_extend
        batch.seq_lens_sum = batch.seq_lens.sum().item()
        batch.req_pool_indices = batch.spec_info.req_pool_indices_for_draft_extend
        batch.return_logprob = False
        batch.return_hidden_states = False

        self.capture_hidden_mode = CaptureHiddenMode.LAST
        self.accept_length = self.accept_length + 1
        self.positions = jnp.empty_like(batch.input_ids, dtype=jnp.int32)
        self.verified_id = jnp.empty_like(self.accept_length, dtype=jnp.int32)

        self.positions, self.verified_id = create_extend_after_decode_spec_info(
            batch.input_ids,
            batch.seq_lens,
            self.accept_length,
            self.positions,
            self.verified_id,
        )

        self.accept_length_cpu = jnp.asarray(accept_length_cpu_host, dtype=jnp.int32)

    def generate_attn_arg_prefill(
        self,
        req_pool_indices: jax.Array,
        paged_kernel_lens: jax.Array,
        paged_kernel_lens_sum: int,
        req_to_token: jax.Array,
    ):
        pass

    def filter_batch(self, new_indices: jax.Array, has_been_filtered: bool = True):
        if has_been_filtered:
            # in eagle_utils.py:verify, we have already filtered the batch by `unfinished_index`
            # therefore, we don't need to filter the batch again in scheduler
            if len(new_indices) != len(self.topk_p):
                logger.warning(
                    "length of new_indices: %d != length of topk_p: %d, this should not happen",
                    len(new_indices),
                    len(self.topk_p),
                )
            self.topk_p = self.topk_p[: len(new_indices)]
            self.topk_index = self.topk_index[: len(new_indices)]
            self.hidden_states = self.hidden_states[: len(new_indices)]
            self.verified_id = self.verified_id[: len(new_indices)]
        else:
            # in some cases(e.g draft_extend), we have not filtered the batch by `unfinished_index`
            self.topk_p = self.topk_p[new_indices]
            self.topk_index = self.topk_index[new_indices]
            self.hidden_states = self.hidden_states[new_indices]
            self.verified_id = self.verified_id[new_indices]

    def merge_batch(self, spec_info: EagleDraftInput):
        if self.hidden_states is None:
            self.hidden_states = spec_info.hidden_states
            self.verified_id = spec_info.verified_id
            self.topk_p = spec_info.topk_p
            self.topk_index = spec_info.topk_index
            return
        if spec_info.hidden_states is None:
            return
        self.hidden_states = jnp.concatenate([self.hidden_states, spec_info.hidden_states], axis=0)
        self.verified_id = jnp.concatenate([self.verified_id, spec_info.verified_id], axis=0)
        self.topk_p = jnp.concatenate([self.topk_p, spec_info.topk_p])
        self.topk_index = jnp.concatenate([self.topk_index, spec_info.topk_index])


@dataclass
class EagleVerifyOutput:
    # Draft input batch
    draft_input: EagleDraftInput
    # Logit outputs from target worker
    logits_output: LogitsProcessorOutput
    # Accepted token ids including the bonus token
    verified_id: jax.Array
    # Accepted token length per sequence in a batch in CPU.
    accept_length_per_req_cpu: list[int]
    # Accepted indices from logits_output.next_token_logits
    accepted_indices: jax.Array


@register_pytree_node_class
@dataclass
class EagleVerifyInput:
    # container type for pytree
    draft_token: jax.Array
    custom_mask: jax.Array
    positions: jax.Array
    retrive_index: jax.Array
    retrive_next_token: jax.Array
    retrive_next_sibling: jax.Array
    retrive_cum_len: jax.Array
    seq_lens_cpu: jax.Array
    # common type for pytree
    spec_steps: int
    topk: int
    draft_token_num: int
    seq_lens_sum: int
    capture_hidden_mode: CaptureHiddenMode
    # grammar: BaseGrammarObject = None

    def tree_flatten(self):
        seq_lens_sum_arr = _as_int32_array(self.seq_lens_sum, fallback=0)

        children = (
            self.draft_token,
            self.custom_mask,
            self.positions,
            self.retrive_index,
            self.retrive_next_token,
            self.retrive_next_sibling,
            self.retrive_cum_len,
            self.seq_lens_cpu,
            seq_lens_sum_arr,
        )

        aux_data = {
            "spec_steps": self.spec_steps,
            "topk": self.topk,
            "draft_token_num": self.draft_token_num,
            "capture_hidden_mode": self.capture_hidden_mode,
        }
        return (children, aux_data)

    @classmethod
    def tree_unflatten(cls, aux_data, children):
        obj = cls.__new__(cls)
        obj.spec_steps = aux_data["spec_steps"]
        obj.topk = aux_data["topk"]
        obj.draft_token_num = aux_data["draft_token_num"]
        obj.capture_hidden_mode = aux_data["capture_hidden_mode"]

        obj.draft_token = children[0]
        obj.custom_mask = children[1]
        obj.positions = children[2]
        obj.retrive_index = children[3]
        obj.retrive_next_token = children[4]
        obj.retrive_next_sibling = children[5]
        obj.retrive_cum_len = children[6]
        obj.seq_lens_cpu = children[7]
        obj.seq_lens_sum = children[8]

        return obj

    def prepare_for_verify(self, batch: ScheduleBatch, page_size: int):
        if batch.forward_mode.is_idle():
            return

        # TODO: keep draft_token on TPU
        batch.input_ids = self.draft_token

        # bs = batch.batch_size()
        prefix_lens = batch.seq_lens
        seq_lens_with_draft_token = batch.seq_lens + self.draft_token_num
        # extend_lens = jnp.array([self.draft_token_num] * bs)

        if page_size == 1:
            batch.out_cache_loc = batch.alloc_token_slots(len(batch.input_ids))
        else:
            last_loc = get_last_loc(
                batch.req_to_token_pool.req_to_token,
                batch.req_pool_indices,
                prefix_lens,
            )
            batch.out_cache_loc = batch.alloc_paged_token_slots_extend(
                prefix_lens.tolist(),
                seq_lens_with_draft_token.tolist(),
                last_loc.tolist(),
                len(batch.input_ids),
            )
            self.last_loc = last_loc

        assign_req_to_token_pool(
            batch.req_pool_indices,
            batch.req_to_token_pool,
            batch.seq_lens,
            seq_lens_with_draft_token,
            batch.out_cache_loc,
        )

    def verify(
        self,
        batch: ScheduleBatch,
        logits_output: LogitsProcessorOutput,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        page_size: int,
        rng: nnx.Rngs,
        vocab_mask: jax.Array | None = None,  # For grammar
    ) -> jax.Array:
        """
        Verify and find accepted tokens based on logits output and batch
        (which contains spec decoding information).

        WARNING: This API in-place modifies the states of logits_output

        This API updates values inside logits_output based on the accepted
        tokens. I.e., logits_output.next_token_logits only contains
        accepted token logits.
        """
        if batch.forward_mode.is_idle():
            return EagleVerifyOutput(
                draft_input=EagleDraftInput.create_idle_input(
                    hidden_size=batch.model_config.hidden_size,
                    dtype=batch.model_config.dtype,
                    topk=self.topk,
                    capture_hidden_mode=CaptureHiddenMode.LAST,
                ),
                logits_output=logits_output,
                verified_id=jnp.empty(0, dtype=jnp.int32),
                accept_length_per_req_cpu=[],
                accepted_indices=jnp.full(
                    (0, self.spec_steps + 1),
                    -1,
                    dtype=jnp.int32,
                ),
            )

        bs = self.retrive_index.shape[0]
        candidates = self.draft_token.reshape(bs, self.draft_token_num)
        sampling_info = batch.sampling_info

        predict_shape = list(logits_output.next_token_logits.shape)[:-1]
        predict_shape[-1] += 1
        predict = jnp.empty(predict_shape, dtype=jnp.int32)

        accept_index = jnp.full((bs, self.spec_steps + 1), -1, dtype=jnp.int32)
        accept_length = jnp.empty((bs,), dtype=jnp.int32)

        if bs != len(sampling_info):
            sampling_info = copy.deepcopy(sampling_info)
            # NOTE: retrive_index are the indices of the requests that are kept.
            sampling_info.filter_batch(self.retrive_index.tolist(), self.retrive_index)

        # TODO: support custom sampler, apply the custom logit processors if registered in the sampling info.
        # if sampling_info.has_custom_logit_processor:
        #     pass
        # TODO: Apply penalty
        # if sampling_info.penalizer_orchestrator.is_required:
        #     pass
        # TODO: Apply grammar mask
        # if vocab_mask is not None:
        #     pass

        # Sample tokens. Force greedy sampling on AMD
        is_all_greedy = sampling_info.is_all_greedy

        if is_all_greedy:
            target_predict = jnp.argmax(logits_output.next_token_logits, axis=-1).flatten()
            accept_index, accept_length, predict = verify_tree_greedy(
                predicts=predict,  # mutable
                accept_index=accept_index,  # mutable
                accept_token_num=accept_length,  # mutable
                candidates=candidates,
                retrive_index=self.retrive_index,
                retrive_next_token=self.retrive_next_token,
                retrive_next_sibling=self.retrive_next_sibling,
                target_predict=target_predict,
            )
        else:

            # apply temperature and get target probs
            expanded_temperature = jnp.repeat(
                sampling_info.temperatures, self.draft_token_num
            )  # (bs * draft_token_num, 1)
            expanded_temperature = jnp.expand_dims(expanded_temperature, axis=-1)
            target_probs = jax.nn.softmax(
                logits_output.next_token_logits / expanded_temperature, axis=-1
            )  # (bs * draft_token_num, vocab_size)
            target_probs = top_k_renorm_prob(
                target_probs, jnp.repeat(sampling_info.top_ks, self.draft_token_num)
            )

            if not jnp.all(sampling_info.top_ps == 1.0):
                target_probs = top_p_renorm_prob(
                    target_probs, jnp.repeat(sampling_info.top_ps, self.draft_token_num)
                )

            # TODO: optimize top_k and top_p by avoiding sort
            rngs = jax.random.split(rng.params(), 3)

            draft_probs = jnp.zeros(target_probs.shape, dtype=jnp.float32)

            # coins for rejection sampling
            coins = jax.random.uniform(rngs[1], candidates.shape, dtype=jnp.float32)
            # coins for final sampling
            coins_for_final_sampling = jax.random.uniform(rngs[2], (bs,), dtype=jnp.float32)
            accept_index, accept_length, predict = tree_speculative_sampling_target_only(
                predicts=predict,
                accept_index=accept_index,
                accept_token_num=accept_length,
                candidates=candidates,
                retrive_index=self.retrive_index,
                retrive_next_token=self.retrive_next_token,
                retrive_next_sibling=self.retrive_next_sibling,
                uniform_samples=coins,
                uniform_samples_for_final_sampling=coins_for_final_sampling,
                target_probs=target_probs,
                draft_probs=draft_probs,
                threshold_single=global_server_args_dict["speculative_accept_threshold_single"],
                threshold_acc=global_server_args_dict["speculative_accept_threshold_acc"],
                deterministic=True,
            )

        if SIMULATE_ACC_LEN:
            # Do simulation
            _, rng = jax.random.split(rng.params())
            accept_index, accept_length, predict = _generate_simulated_accept_index(
                accept_index=accept_index,
                predict=predict,
                accept_length=accept_length,
                simulate_acc_len=SIMULATE_ACC_LEN,
                bs=bs,
                spec_steps=self.spec_steps,
                rng=rng,
            )

        unfinished_index = []
        unfinished_accept_index = []
        accept_index_cpu = accept_index.tolist()
        predict_cpu = predict.tolist()
        has_finished = False

        # Iterate every accepted token and check if req has finished after append the token
        # should be checked BEFORE free kv cache slots
        for i, (req, accept_index_row) in enumerate(zip(batch.reqs, accept_index_cpu)):
            for j, idx in enumerate(accept_index_row):
                if idx == -1:
                    break
                id = predict_cpu[idx]
                req.output_ids.append(id)
                req.check_finished()
                if req.finished():
                    has_finished = True
                    # set all tokens after finished token to -1 and break
                    accept_index = accept_index.at[i, j + 1 :].set(-1)
                    break
            if not req.finished():
                unfinished_index.append(i)
                if idx == -1:
                    unfinished_accept_index.append(accept_index[i, :j])
                else:
                    unfinished_accept_index.append(accept_index[i])
            req.spec_verify_ct += 1

        if has_finished:
            accept_length = (accept_index != -1).sum(axis=1) - 1

        # Free the KV cache for unaccepted tokens
        # TODO: fuse them
        accept_index = accept_index[accept_index != -1]
        verified_id = predict[accept_index]
        evict_mask = jnp.full_like(self.draft_token, True, dtype=jnp.bool)
        evict_mask = evict_mask.at[accept_index].set(False)

        if page_size == 1:
            # TODO: boolean array index leads to a device sync. Remove it.
            token_to_kv_pool_allocator.free(batch.out_cache_loc[evict_mask])
        else:
            if self.topk == 1:
                # Only evict full empty page. Do not evict partial empty page
                evict_mask = align_evict_mask_to_page_size(
                    batch.seq_lens,
                    evict_mask,
                    page_size,
                    self.draft_token_num,
                )
                token_to_kv_pool_allocator.free(batch.out_cache_loc[evict_mask])
            else:
                # Shift the accepted tokens to the beginning.
                # Only evict the last part
                src_cache_loc, tgt_cache_loc, to_free_num_slots = get_src_tgt_cache_loc(
                    batch.seq_lens,
                    batch.out_cache_loc,
                    accept_index,
                    accept_length,
                    self.draft_token_num,
                    page_size,
                )

                # out_cache_loc: [0  1  2,  3  4  5,  6  7  8]
                # accept_index:  [0  1 -1,  3  4 -1,  6 -1 -1]
                # tgt_cache_loc: [0  1   ,  3  4   ,  6      ]
                # to_free_slots: [      2,        5,     7  8]
                # to_free_slots also needs to be page-aligned without the first partial page
                #
                # split each row of out_cache_loc into two parts.
                # 1. the first part goes to tgt_cache_loc. length = accept_length[i] + 1
                # 2. the second part goes to to_free_slots.
                tgt_cache_loc, to_free_slots = get_target_cache_loc(
                    accept_length,
                    to_free_num_slots,
                    batch.out_cache_loc,
                    self.draft_token_num,
                )

                # Free the kv cache
                token_to_kv_pool_allocator.free(to_free_slots)

                # Copy the kv cache
                batch.token_to_kv_pool_allocator.get_kvcache().move_kv_cache(
                    tgt_cache_loc, src_cache_loc
                )

        # Construct EagleVerifyOutput
        if not has_finished:
            if page_size == 1 or self.topk == 1:
                batch.out_cache_loc = batch.out_cache_loc[accept_index]
                assign_req_to_token_pool(
                    batch.req_pool_indices,
                    batch.req_to_token_pool,
                    batch.seq_lens,
                    batch.seq_lens + accept_length + 1,
                    batch.out_cache_loc,
                )
            else:
                batch.out_cache_loc = tgt_cache_loc
            batch.seq_lens = batch.seq_lens + (accept_length + 1)

            accept_length_cpu_host = numpy.asarray(jax.device_get(accept_length), dtype=numpy.int32)
            draft_input = EagleDraftInput(
                hidden_states=batch.spec_info.hidden_states[accept_index],
                verified_id=verified_id,
                accept_length=accept_length,
                accept_length_cpu=accept_length,
                seq_lens_for_draft_extend=batch.seq_lens,
                req_pool_indices_for_draft_extend=batch.req_pool_indices,
            )

            return EagleVerifyOutput(
                draft_input=draft_input,
                logits_output=logits_output,
                verified_id=verified_id,
                accept_length_per_req_cpu=accept_length_cpu_host.tolist(),
                accepted_indices=accept_index,
            )
        else:
            if page_size == 1 or self.topk == 1:
                assign_req_to_token_pool(
                    batch.req_pool_indices,
                    batch.req_to_token_pool,
                    batch.seq_lens,
                    batch.seq_lens + accept_length + 1,
                    batch.out_cache_loc[accept_index],
                )
                batch.seq_lens = batch.seq_lens + (accept_length + 1)

            accept_length_cpu_host = numpy.asarray(jax.device_get(accept_length), dtype=numpy.int32)
            accept_length_cpu = accept_length_cpu_host.tolist()
            if len(unfinished_accept_index) > 0:
                unfinished_accept_index = jnp.concatenate(unfinished_accept_index)
                unfinished_index_device = jnp.array(unfinished_index, dtype=jnp.int32)
                draft_input_accept_length_cpu = [accept_length_cpu[i] for i in unfinished_index]
                if page_size == 1 or self.topk == 1:
                    batch.out_cache_loc = batch.out_cache_loc[unfinished_accept_index]
                else:
                    batch.out_cache_loc = jnp.empty(
                        len(unfinished_index) + sum(draft_input_accept_length_cpu),
                        dtype=jnp.int32,
                    )
                    accept_length_filter = create_accept_length_filter(
                        accept_length,
                        unfinished_index_device,
                        batch.seq_lens,
                    )
                    batch.out_cache_loc = filter_finished_cache_loc_kernel(
                        tgt_cache_loc,
                        accept_length,
                        accept_length_filter,
                    )

                draft_input = EagleDraftInput(
                    hidden_states=batch.spec_info.hidden_states[unfinished_accept_index],
                    verified_id=predict[unfinished_accept_index],
                    accept_length_cpu=jnp.asarray(draft_input_accept_length_cpu, dtype=jnp.int32),
                    accept_length=accept_length[unfinished_index_device],
                    seq_lens_for_draft_extend=batch.seq_lens[unfinished_index_device],
                    req_pool_indices_for_draft_extend=batch.req_pool_indices[
                        unfinished_index_device
                    ],
                )
            else:
                draft_input = EagleDraftInput.create_idle_input(
                    hidden_size=batch.model_config.hidden_size,
                    dtype=batch.model_config.dtype,
                    topk=self.topk,
                    capture_hidden_mode=CaptureHiddenMode.LAST,
                )

            return EagleVerifyOutput(
                draft_input=draft_input,
                logits_output=logits_output,
                verified_id=verified_id,
                accept_length_per_req_cpu=accept_length_cpu,
                accepted_indices=accept_index,
            )


def _generate_simulated_accept_index(
    accept_index: jax.Array,
    predict,
    accept_length,
    simulate_acc_len,
    bs,
    spec_steps,
    rng: nnx.Rngs,
):
    simulate_acc_len_float = float(simulate_acc_len)
    if SIMULATE_ACC_METHOD == "multinomial":
        # here data is on cpu
        simulated_values = numpy.random.normal(
            loc=simulate_acc_len_float,
            scale=1.0,
            size=(1,),
        )
        # clamp simulated values to be between 1 and self.spec_steps
        simulated_values = jnp.clip(simulated_values, min=1.0, max=spec_steps + 1)
        simulate_acc_len = int(simulated_values.round().item())
    elif SIMULATE_ACC_METHOD == "match-expected":
        # multinomial sampling does not match the expected length
        # we keep it for the sake of compatibility of existing tests
        # but it's better to use "match-expected" for the cases that need to
        # match the expected length, One caveat is that this will only sample
        # either round down or round up of the expected length
        simulate_acc_len_float = max(1.0, min(spec_steps + 1, simulate_acc_len_float))
        lower = int(simulate_acc_len_float // 1)
        upper = lower + 1 if lower < spec_steps + 1 else lower
        if lower == upper:
            simulate_acc_len = lower
        else:
            weight_upper = simulate_acc_len_float - lower
            weight_lower = 1.0 - weight_upper
            # here, data is on cpu
            probs = jnp.array([weight_lower, weight_upper])
            sampled_index = jax.random.categorical(rng, jnp.log(probs))
            simulate_acc_len = lower if sampled_index == 0 else upper
    else:
        raise ValueError(f"Invalid simulate_acc_method: {SIMULATE_ACC_METHOD}")

    accept_indx_first_col = accept_index[:, 0].reshape(-1, 1)
    sim_accept_index = jnp.full((bs, spec_steps + 1), -1, dtype=jnp.int32)
    sim_accept_index = sim_accept_index.at[:, :simulate_acc_len].set(
        accept_indx_first_col + jnp.arange(simulate_acc_len)
    )
    accept_length = accept_length.at[:].set(simulate_acc_len - 1)
    predict = predict.at[:].set(100)  # some legit token id
    return sim_accept_index, accept_length, predict


def build_eagle_tree_structure(
    parent_list: jax.Array,
    selected_index: jax.Array,
    verified_seq_len: jax.Array,
    bs: int,
    draft_token_num: int,
    topk: int,
    depth: int,
    seq_lens_sum: int,
    tree_mask_mode: int = 0,  # FULL_MASK = 0
    max_seq_len_per_req: int | None = None,
    max_tree_mask_size: int | None = None,
    actual_tree_mask_size: int | None = None,
) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:
    """
    Args:
        parent_list: Parent indices array [bs, topk * (depth-1) + 1]
        selected_index: Selected token indices [bs, draft_token_num - 1]
        verified_seq_len: Sequence lengths [bs]
        bs: Batch size
        draft_token_num: Number of draft tokens (num_verify_tokens)
        topk: Top-k value
        depth: Tree depth
        seq_lens_sum: Sum of sequence lengths
        tree_mask_mode: Tree mask mode (0=FULL_MASK)
        max_seq_len_per_req: Static upper bound for sequence length per request
        max_tree_mask_size: Optional explicit capacity for the tree mask buffer
        actual_tree_mask_size: Optional override for the exact number of valid mask entries

    Returns:
        tuple of (tree_mask, positions, retrive_index, retrive_next_token, retrive_next_sibling)
    """

    if tree_mask_mode == 0:  # FULL_MASK
        inferred_actual_size = (
            seq_lens_sum * draft_token_num + draft_token_num * draft_token_num * bs
        )
        tree_mask_size = (
            inferred_actual_size if actual_tree_mask_size is None else actual_tree_mask_size
        )
        inferred_capacity = (
            max_seq_len_per_req * draft_token_num * bs + draft_token_num * draft_token_num * bs
            if max_seq_len_per_req is not None
            else inferred_actual_size
        )
        tree_mask_capacity = inferred_capacity if max_tree_mask_size is None else max_tree_mask_size
    else:
        inferred_actual_size = bs * draft_token_num * draft_token_num
        tree_mask_size = (
            inferred_actual_size if actual_tree_mask_size is None else actual_tree_mask_size
        )
        tree_mask_capacity = (
            inferred_actual_size if max_tree_mask_size is None else max_tree_mask_size
        )

    tree_mask = jnp.zeros((tree_mask_capacity,), dtype=jnp.bool_)
    if tree_mask_size > 0:
        tree_mask = tree_mask.at[:tree_mask_size].set(True)
    positions = jnp.zeros((bs * draft_token_num,), dtype=jnp.int32)
    retrive_index = jnp.full((bs, draft_token_num), -1, dtype=jnp.int32)
    retrive_next_token = jnp.full((bs, draft_token_num), -1, dtype=jnp.int32)
    retrive_next_sibling = jnp.full((bs, draft_token_num), -1, dtype=jnp.int32)

    for bid in range(bs):
        seq_len = verified_seq_len[bid]

        # Calculate seq_tree_idx for this batch (exactly like CUDA kernel)
        seq_tree_idx = draft_token_num * draft_token_num * bid
        if tree_mask_mode == 0:  # FULL_MASK
            for i in range(bid):
                seq_tree_idx += verified_seq_len[i] * draft_token_num
        for tid in range(draft_token_num):
            global_token_idx = bid * draft_token_num + tid

            # Calculate token_tree_idx for tree_mask
            if tree_mask_mode == 0:  # FULL_MASK
                token_tree_idx = seq_tree_idx + (seq_len + draft_token_num) * tid + seq_len + 1
            else:
                token_tree_idx = draft_token_num * draft_token_num * bid + draft_token_num * tid + 1

            # Set tree_mask for this token
            if token_tree_idx > 0 and token_tree_idx <= tree_mask_size:
                tree_mask = tree_mask.at[token_tree_idx - 1].set(True)

            # Clear next draft_token_num - 1 positions
            for i in range(draft_token_num - 1):
                mask_idx = token_tree_idx + i
                if mask_idx < tree_mask_size:
                    tree_mask = tree_mask.at[mask_idx].set(False)

            if tid == 0:
                # Verified token (tid == 0)
                positions = positions.at[global_token_idx].set(seq_len)
                retrive_index = retrive_index.at[bid, tid].set(global_token_idx)

                # Build retrive_next_token and retrive_next_sibling (backwards iteration)
                retrive_index_offset = bid * draft_token_num
                for i in range(draft_token_num - 1, 0, -1):  # i from draft_token_num-1 to 1
                    current_token_idx = retrive_index_offset + i
                    retrive_index = retrive_index.at[bid, i].set(current_token_idx)

                    selected_idx = bid * (draft_token_num - 1) + i - 1
                    parent_tb_idx = selected_index.flatten()[selected_idx] // topk
                    parent_position = 0

                    if parent_tb_idx > 0:
                        parent_list_idx = bid * (topk * (depth - 1) + 1) + parent_tb_idx
                        if parent_list_idx < parent_list.size:
                            parent_token_idx = parent_list.flatten()[parent_list_idx]

                            for parent_pos in range(draft_token_num - 1):
                                check_idx = bid * (draft_token_num - 1) + parent_pos
                                if (
                                    check_idx < selected_index.size
                                    and selected_index.flatten()[check_idx] == parent_token_idx
                                ):
                                    parent_position = parent_pos + 1  # +1 to convert to 1-indexed
                                    break
                            else:
                                parent_position = draft_token_num  # Not found
                        else:
                            parent_position = draft_token_num  # Invalid parent_list_idx
                    else:
                        parent_position = 0  # Root node

                    if parent_position >= draft_token_num:
                        # Invalid parent, skip
                        continue

                    next_token_idx = bid * draft_token_num + parent_position
                    if retrive_next_token.flatten()[next_token_idx] == -1:
                        retrive_next_token = retrive_next_token.at[bid, parent_position].set(i)
                    else:
                        # There's already a next_token, so set sibling
                        origin_next_token = retrive_next_token.flatten()[next_token_idx]
                        retrive_next_token = retrive_next_token.at[bid, parent_position].set(i)
                        retrive_next_sibling = retrive_next_sibling.at[bid, i].set(
                            origin_next_token
                        )

                retrive_index = retrive_index.at[bid, 0].set(bid * draft_token_num)

            else:
                # Draft token (tid > 0)
                # Calculate position by tracing back to root
                position = 0
                cur_position = tid - 1  # Convert to 0-indexed for selected_index

                while True:
                    position += 1
                    mask_idx = token_tree_idx + cur_position
                    if mask_idx < tree_mask_size:
                        tree_mask = tree_mask.at[mask_idx].set(True)

                    selected_idx = bid * (draft_token_num - 1) + cur_position
                    parent_tb_idx = selected_index.flatten()[selected_idx] // topk

                    if parent_tb_idx == 0:
                        # Reached root
                        break

                    parent_list_idx = bid * (topk * (depth - 1) + 1) + parent_tb_idx
                    if parent_list_idx < parent_list.size:
                        token_idx = parent_list.flatten()[parent_list_idx]

                        found = False
                        for cur_pos in range(draft_token_num - 1):
                            check_idx = bid * (draft_token_num - 1) + cur_pos
                            if (
                                check_idx < selected_index.size
                                and selected_index.flatten()[check_idx] == token_idx
                            ):
                                cur_position = cur_pos
                                found = True
                                break

                        if not found:
                            break  # Invalid tree structure
                    else:
                        break  # Invalid parent_list_idx

                positions = positions.at[global_token_idx].set(position + seq_len)
                retrive_index = retrive_index.at[bid, tid].set(global_token_idx)

    return tree_mask, positions, retrive_index, retrive_next_token, retrive_next_sibling


def get_src_tgt_cache_loc(
    seq_lens: jax.Array,
    out_cache_loc: jax.Array,
    accept_index: jax.Array,
    accept_length: jax.Array,
    draft_token_num: int,
    page_size: int,
):
    src_cache_loc = out_cache_loc[accept_index]
    extended_len = seq_lens + draft_token_num
    keep_len = jnp.minimum(
        (seq_lens + accept_length + 1 + page_size - 1) // page_size * page_size,
        extended_len,
    )
    to_free_num_slots = extended_len - keep_len
    return src_cache_loc, to_free_num_slots


def create_accept_length_filter(
    accept_length: jax.Array,
    unfinished_index_device: jax.Array,
    seq_lens: jax.Array,
):
    accept_length_filter = jnp.zeros_like(accept_length)
    accept_length_filter[unfinished_index_device] = accept_length[unfinished_index_device] + 1
    seq_lens.add_(accept_length + 1)
    return accept_length_filter
