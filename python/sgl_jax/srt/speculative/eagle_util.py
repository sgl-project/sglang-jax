from __future__ import annotations

import copy
import functools
import logging
import os
from collections.abc import Sequence
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, ClassVar

import jax
import jax.numpy as jnp
import numpy
import numpy as np
from flax import nnx
from jax.sharding import Mesh
from jax.tree_util import register_pytree_node_class

from sgl_jax.srt.layers.logits_processor import LogitsProcessorOutput

if TYPE_CHECKING:
    from sgl_jax.srt.managers.scheduler import GenerationBatchResult

from sgl_jax.srt.kernels.speculative.build_eagle_tree_structure_kernel import (
    build_eagle_tree_structure,
)
from sgl_jax.srt.kernels.speculative.kernel import (
    create_extend_after_decode_spec_info,
    top_k_renorm_prob,
    top_p_renorm_prob,
    tree_speculative_sampling_target_only,
)
from sgl_jax.srt.kernels.speculative.verify_tree_greedy_kernel import verify_tree_greedy
from sgl_jax.srt.managers.schedule_batch import (
    ModelWorkerBatch,
    ScheduleBatch,
    get_last_loc,
    global_server_args_dict,
)
from sgl_jax.srt.managers.tp_worker import ModelWorker
from sgl_jax.srt.mem_cache.common import (
    alloc_paged_token_slots_extend,
    alloc_token_slots,
)
from sgl_jax.srt.model_executor.forward_batch_info import CaptureHiddenMode, ForwardMode

logger = logging.getLogger(__name__)

SIMULATE_ACC_LEN = os.environ.get("SIMULATE_ACC_LEN")
SIMULATE_ACC_METHOD = os.environ.get("SIMULATE_ACC_METHOD", "multinomial")


def _is_jax_leaf(value: Any) -> bool:
    """Detect sentinel nodes generated by jax.tree_util when shaping pytrees."""
    cls = value.__class__
    return cls.__name__ == "Leaf" and cls.__module__.startswith("jax.")


def _as_int32_array(value: Any, *, fallback: int = -1) -> jax.Array:
    """Convert scalar-like inputs into scalar int32 JAX arrays."""
    if value is None:
        return None
    if isinstance(value, jax.Array):
        return value
    if isinstance(value, numpy.ndarray):
        return jnp.asarray(value, dtype=jnp.int32)
    if isinstance(value, (int, numpy.integer)):
        return jnp.asarray(int(value), dtype=jnp.int32)
    if isinstance(value, (list, tuple)):
        return jnp.asarray(value, dtype=jnp.int32)
    if _is_jax_leaf(value):
        return jnp.asarray(fallback, dtype=jnp.int32)
    try:
        return jnp.asarray(value, dtype=jnp.int32)
    except (TypeError, ValueError) as exc:
        raise TypeError(
            f"Unable to convert value of type {type(value)} into int32 metadata array."
        ) from exc


def get_last_loc_jax_array(
    req_to_token: jax.Array,
    req_pool_indices: jax.Array,
    prefix_lens: jax.Array,
) -> jax.Array:
    """JAX version of get_last_loc that operates on JAX arrays.

    Args:
        req_to_token: Token mapping tensor of shape (num_reqs, max_seq_len)
        req_pool_indices: Request pool indices of shape (batch_size,)
        prefix_lens: Prefix lengths of shape (batch_size,)

    Returns:
        Last location tensor of shape (batch_size,)
    """
    return jnp.where(
        prefix_lens > 0,
        req_to_token[req_pool_indices, prefix_lens - 1],
        jnp.full_like(prefix_lens, -1),
    )


def get_last_loc_large_page_size_top_k_1(
    req_to_token: jax.Array,
    req_pool_indices: jax.Array,
    seq_lens: jax.Array,
    speculative_num_steps: int,
) -> tuple[jax.Array, jax.Array, jax.Array]:
    """JAX implementation of get_last_loc_large_page_size_top_k_1.

    This function is used in EAGLE speculative decoding to compute cache locations
    for large page sizes when top_k=1.

    Args:
        req_to_token: Request to token mapping tensor
        req_pool_indices: Request pool indices
        seq_lens: Current sequence lengths
        speculative_num_steps: Number of speculative decoding steps

    Returns:
        tuple of (prefix_lens, new_seq_lens, last_loc):
        - prefix_lens: Same as input seq_lens
        - new_seq_lens: Updated sequence lengths (prefix_lens + speculative_num_steps)
        - last_loc: Last cache locations computed using get_last_loc
    """
    prefix_lens = seq_lens
    new_seq_lens = prefix_lens + speculative_num_steps
    last_loc = get_last_loc_jax_array(
        req_to_token,
        req_pool_indices,
        prefix_lens,
    )
    return prefix_lens, new_seq_lens, last_loc


def get_last_loc_large_page_size_large_top_k(
    req_to_token: jax.Array,
    req_pool_indices: jax.Array,
    seq_lens: jax.Array,
    speculative_num_steps: int,
    topk: int,
    page_size: int,
) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:
    """JAX implementation of get_last_loc_large_page_size_large_top_k.

    This function handles large page sizes with large top_k values in EAGLE speculative decoding.
    It computes cache locations and manages page allocation for multiple top-k branches.

    Args:
        req_to_token: Request to token mapping tensor
        req_pool_indices: Request pool indices
        seq_lens: Current sequence lengths
        speculative_num_steps: Number of speculative decoding steps
        topk: Number of top-k branches
        page_size: Size of each memory page

    Returns:
        tuple of (prefix_lens, new_seq_lens, last_loc, num_new_pages_per_topk, extend_lens):
        - prefix_lens: Same as input seq_lens
        - new_seq_lens: Updated sequence lengths considering page alignment
        - last_loc: Last cache locations
        - num_new_pages_per_topk: Number of new pages needed per top-k branch
        - extend_lens: Number of tokens to extend for each sequence
    """
    prefix_lens = seq_lens
    last_page_lens = prefix_lens % page_size
    num_new_pages_per_topk = (last_page_lens + speculative_num_steps + page_size - 1) // page_size

    new_seq_lens = prefix_lens // page_size * page_size + num_new_pages_per_topk * (
        page_size * topk
    )
    extend_lens = new_seq_lens - prefix_lens

    last_loc = get_last_loc_jax_array(
        req_to_token,
        req_pool_indices,
        prefix_lens,
    )

    return prefix_lens, new_seq_lens, last_loc, num_new_pages_per_topk, extend_lens


@functools.partial(
    jax.jit,
    static_argnums=(
        4,
        5,
        6,
    ),
)
def build_tree_kernel_efficient_preprocess(
    verified_id: jax.Array,
    score_tensor: jax.Array,
    ss_token_list: jax.Array,
    parents_list: jax.Array,
    num_verify_tokens: int,
    topk: int,
    speculative_num_steps: int,
):
    # score_list   (bs, 1 + (step - 1) * topk  , eagle_topk)
    # token_list   (bs, topk + (step - 1) * topk * topk)
    # parents_list (bs, topk + 1 + (step - 1) * topk)
    # score_tensor = jnp.concatenate(score_list, axis=1)
    # FIXME this reshape operation should be optimized
    score_tensor = score_tensor.reshape(score_tensor.shape[0], -1)

    # ss_token_list = jnp.concatenate(token_list, axis=1)

    _, top_scores_index = jax.lax.top_k(score_tensor, num_verify_tokens - 1)
    top_scores_index = jnp.sort(top_scores_index, axis=-1)

    draft_tokens = jnp.take_along_axis(ss_token_list, top_scores_index, axis=1)

    verified_expanded = jnp.expand_dims(verified_id, axis=1)
    # FIXME this as type operation should be optimized
    draft_tokens = (
        jnp.concatenate([verified_expanded, draft_tokens], axis=1).flatten().astype(jnp.int32)
    )
    if speculative_num_steps > 1:
        parent_list = parents_list[:, : (topk + 1 + (speculative_num_steps - 2) * topk)]
    else:
        batch_size = parents_list.shape[0]
        parent_list = jnp.full((batch_size, 1), -1, dtype=jnp.int32)

    return parent_list, top_scores_index, draft_tokens


@functools.partial(
    jax.jit,
    static_argnames=(
        "padded_seq_lens_sum",
        "num_verify_tokens",
        "topk",
        "max_seq_len_per_req",
        "tree_mask_mode",
        "speculative_num_steps",
    ),
)
def _build_tree_kernel_efficient_core(
    verified_id: jax.Array,
    score_list: jax.Array,
    token_list: jax.Array,
    parents_list: jax.Array,
    seq_lens: jax.Array,
    seq_lens_sum: jax.Array,
    *,
    padded_seq_lens_sum: int,
    num_verify_tokens: int,
    topk: int,
    max_seq_len_per_req: int,
    tree_mask_mode: int,
    speculative_num_steps: int,
) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:

    tree_mask, positions, retrive_index, retrive_next_token, retrive_next_sibling = (
        build_eagle_tree_structure(
            parent_list=parents_list,
            selected_index=score_list,
            verified_seq_len=seq_lens,
            draft_token_num=num_verify_tokens,
            topk=topk,
            seq_lens_sum=seq_lens_sum,
            padded_seq_lens_sum=padded_seq_lens_sum,
            tree_mask_mode=tree_mask_mode,
        )
    )

    return (
        tree_mask,
        positions,
        retrive_index,
        retrive_next_token,
        retrive_next_sibling,
        token_list,
    )


def _extract_parent_branch_indices(
    parents_entry: np.ndarray, step_index: int, topk: int
) -> np.ndarray:
    if step_index == 0:
        raise ValueError("Step index 0 has no parents")
    offset = topk if step_index == 1 else topk**2 * (step_index - 1) + topk
    raw = parents_entry.astype(np.int64) - offset
    parent_indices = np.floor_divide(raw, topk)
    parent_indices = np.clip(parent_indices, 0, topk - 1).astype(np.int32)
    return parent_indices


def build_tree_mask_for_draft_decode(
    seq_lens: jax.Array | np.ndarray,
    topk: int,
    speculative_step_id: int,
    parents_list: Sequence[jax.Array],
) -> jax.Array:
    """
    Build flattened custom mask for draft decode that respects branch ancestry.

    Args:
        seq_lens: Sequence lengths (prompt+accepted) for each request.
        topk: Number of speculative branches processed in parallel.
        speculative_step_id: Current speculative step (0-indexed).
        parents_list: List of parent index tensors produced by ``select_top_k_tokens``.

    Returns:
        Flattened boolean mask concatenating ``topk`` rows per request.
    """

    if topk <= 0:
        raise ValueError("topk must be positive")

    seq_lens_np = np.asarray(seq_lens, dtype=np.int32)
    bs = seq_lens_np.shape[0]
    if speculative_step_id + 1 > len(parents_list):
        raise ValueError("parents_list must contain at least speculative_step_id + 1 entries")

    # Precompute ancestry mapping: path[step, bid, branch]
    ancestry = np.zeros((speculative_step_id + 1, bs, topk), dtype=np.int32)
    ancestry[speculative_step_id] = np.broadcast_to(np.arange(topk, dtype=np.int32), (bs, topk))

    for step in range(speculative_step_id, 0, -1):
        parents_entry = np.asarray(parents_list[step])
        parent_indices = _extract_parent_branch_indices(parents_entry, step, topk)
        for bid in range(bs):
            child_branch_ids = ancestry[step, bid]
            ancestry[step - 1, bid] = parent_indices[bid, child_branch_ids]

    masks: list[np.ndarray] = []
    for bid in range(bs):
        seq_len = int(seq_lens_np[bid])
        kv_len = seq_len + (speculative_step_id + 1) * topk
        mask = np.zeros((topk, kv_len), dtype=np.bool_)
        mask[:, :seq_len] = True

        for branch in range(topk):
            for step in range(speculative_step_id + 1):
                branch_idx = ancestry[step, bid, branch]
                position = seq_len + step * topk + branch_idx
                mask[branch, position] = True

        masks.append(mask.reshape(-1))

    if not masks:
        return jnp.zeros((0,), dtype=jnp.bool_)

    concatenated = np.concatenate(masks)
    return jnp.asarray(concatenated, dtype=jnp.int32)


def build_tree_kernel_efficient(
    verified_id: jax.Array,
    score_list: jax.Array,
    token_list: jax.Array,
    parents_list: jax.Array,
    seq_lens: jax.Array,
    seq_lens_sum: jax.Array,
    padded_seq_lens_sum: int,
    topk: int,
    num_verify_tokens: int,
    max_seq_len_per_req: int,
    speculative_num_steps: int,
    mesh: Mesh,
) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:
    """JAX implementation of build_tree_kernel_efficient.

    Args:
        verified_id: Verified token IDs from previous step
        score_list: List of score tensors from draft model
        token_list: List of token tensors from draft model
        parents_list: List of parent index tensors
        seq_lens: Sequence lengths
        seq_lens_sum: Sum of sequence lengths
        topk: Number of top-k candidates
        num_verify_tokens: Number of tokens to verify
        max_seq_len_per_req: Maximum allowed sequence length per request (static bound)

    Returns:
        tuple of (tree_mask, positions, retrive_index, retrive_next_token,
                 retrive_next_sibling, draft_tokens)
    """
    parent_list, top_scores_index, draft_tokens = build_tree_kernel_efficient_preprocess(
        verified_id,
        score_list,
        token_list,
        parents_list,
        num_verify_tokens,
        topk,
        speculative_num_steps,
    )

    # Get batch size
    # for compatibility, 0.6.3 need to use use_mesh. set_mesh is not have __entry__ attribute.
    # on jax >=0.7.1, we need to use set_mesh.
    try:
        ctx = jax.sharding.use_mesh(mesh)
    except AttributeError:
        try:
            ctx = jax.set_mesh(mesh)
        except AttributeError:
            ctx = mesh
    with ctx:
        (
            tree_mask,
            positions,
            retrive_index,
            retrive_next_token,
            retrive_next_sibling,
            draft_tokens,
        ) = _build_tree_kernel_efficient_core(
            verified_id,
            top_scores_index,
            draft_tokens,
            parent_list,
            seq_lens,
            seq_lens_sum,
            padded_seq_lens_sum=padded_seq_lens_sum,
            num_verify_tokens=num_verify_tokens,
            topk=topk,
            max_seq_len_per_req=max_seq_len_per_req,
            tree_mask_mode=0,
            speculative_num_steps=speculative_num_steps,
        )

    return (
        tree_mask,
        positions,
        retrive_index,
        retrive_next_token,
        retrive_next_sibling,
        draft_tokens,
    )


@register_pytree_node_class
@dataclass
class EagleDraftInput:
    # Constant: alloc length per decode step
    ALLOC_LEN_PER_DECODE: ClassVar[int] = None

    # The inputs for decode
    # shape: (b, topk)
    topk_p: np.ndarray = None
    topk_index: np.ndarray = None
    # shape: (b, hidden_size)
    hidden_states: np.ndarray = None
    capture_hidden_mode: CaptureHiddenMode = CaptureHiddenMode.FULL

    # Inputs for extend
    # shape: (b,)
    verified_id: np.ndarray = None
    accept_length: np.ndarray = None
    accept_length_cpu: np.ndarray | None = None

    # Inputs for the attention backends
    # shape: (b + 1,)
    kv_indptr: np.ndarray = None
    kv_indices: np.ndarray = None

    # Shape info for padding
    num_tokens_per_batch: int = -1
    num_tokens_for_logprob_per_batch: int = -1

    # Inputs for draft extend
    # shape: (b,)
    seq_lens_for_draft_extend: np.ndarray = None
    req_pool_indices_for_draft_extend: np.ndarray = None

    # Inputs for V2 overlap worker
    # future_indices: Optional[FutureIndices] = None
    allocate_lens: np.ndarray | None = None
    new_seq_lens: np.ndarray | None = None
    # verify_done: Optional[torch.cuda.Event] = None

    def tree_flatten(self):
        accept_length_cpu_arr = (
            jnp.empty((0,), dtype=jnp.int32)
            if self.accept_length_cpu is None
            else _as_int32_array(self.accept_length_cpu, fallback=0)
        )

        num_tokens_per_batch_arr = _as_int32_array(self.num_tokens_per_batch)
        num_tokens_for_logprob_arr = _as_int32_array(self.num_tokens_for_logprob_per_batch)

        children = (
            self.topk_p,
            self.topk_index,
            self.hidden_states,
            self.verified_id,
            self.accept_length,
            self.kv_indptr,
            self.kv_indices,
            self.seq_lens_for_draft_extend,
            self.req_pool_indices_for_draft_extend,
            accept_length_cpu_arr,
            num_tokens_per_batch_arr,
            num_tokens_for_logprob_arr,
        )

        aux_data = {
            "capture_hidden_mode": self.capture_hidden_mode,
        }
        return (children, aux_data)

    @classmethod
    def tree_unflatten(cls, aux_data, children):
        obj = cls.__new__(cls)
        obj.capture_hidden_mode = aux_data["capture_hidden_mode"]
        obj.topk_p = children[0]
        obj.topk_index = children[1]
        obj.hidden_states = children[2]
        obj.verified_id = children[3]
        obj.accept_length = children[4]
        obj.kv_indptr = children[5]
        obj.kv_indices = children[6]
        obj.seq_lens_for_draft_extend = children[7]
        obj.req_pool_indices_for_draft_extend = children[8]

        obj.accept_length_cpu = children[9]
        obj.num_tokens_per_batch = children[10]
        obj.num_tokens_for_logprob_per_batch = children[11]

        return obj

    def prepare_for_extend_after_target_prefill(self, model_worker_batch: ModelWorkerBatch):

        if model_worker_batch.forward_mode.is_idle():
            return

        # Prefill only generate 1 token.
        assert (
            self.verified_id.shape[0] == model_worker_batch.real_bs
        ), f"{self.verified_id.shape=} {model_worker_batch.real_bs=}"

        pt = 0
        for i in range(model_worker_batch.real_bs):
            extend_len = model_worker_batch.extend_seq_lens[i]
            input_ids = model_worker_batch.input_ids[pt : pt + extend_len]

            # TODO: batch.input_ids should on tpu
            model_worker_batch.input_ids[pt : pt + extend_len] = np.concatenate(
                (input_ids[1:], self.verified_id[i].reshape(1))
            )
            pt += extend_len

    def prepare_for_extend_after_verify(
        self,
        model_worker_batch: ModelWorkerBatch,
        predict: np.ndarray,
        num_draft_tokens: int,
        draft_model_runner: Any,
        batch_output: GenerationBatchResult,
        precompile_token_paddings,
        precompile_bs_paddings,
        precompile_cache_loc_paddings,
    ):
        # seq_lens_cpu_ = model_worker_batch.seq_lens

        model_worker_batch.spec_info = self
        verified_id = batch_output.next_draft_input.verified_id
        verified_id = verified_id[verified_id != 0].flatten()
        model_worker_batch.input_ids = np.array(jax.device_get(verified_id), dtype=np.int32)
        model_worker_batch.seq_lens = np.array(
            jax.device_get(
                model_worker_batch.seq_lens[: model_worker_batch.real_bs] + batch_output.accept_lens
            )
        )
        model_worker_batch.extend_seq_lens = np.asarray(
            jax.device_get(
                [batch_output.accept_lens[i] for i in range(batch_output.accept_lens.shape[0])]
            )
        )
        model_worker_batch.capture_hidden_mode = CaptureHiddenMode.LAST
        model_worker_batch.spec_info.capture_hidden_mode = CaptureHiddenMode.LAST
        model_worker_batch.forward_mode = ForwardMode.DRAFT_EXTEND
        model_worker_batch.spec_info.hidden_states = np.array(
            jax.device_get(batch_output.next_draft_input.hidden_states)
        )
        forward_metadata = draft_model_runner.attn_backend.get_eagle_forward_metadata(
            model_worker_batch
        )
        draft_model_runner.attn_backend.forward_metadata = forward_metadata
        from sgl_jax.srt.layers.logits_processor import LogitsMetadata

        logits_metadata = LogitsMetadata.from_model_worker_batch(
            model_worker_batch, draft_model_runner.mesh
        )
        model_worker_batch.padding_model_worker_batch(
            precompile_token_paddings, precompile_bs_paddings, precompile_cache_loc_paddings
        )
        hidden_states = model_worker_batch.spec_info.hidden_states
        if (
            hidden_states is not None
            and hidden_states.shape[0] < model_worker_batch.input_ids.shape[0]
        ):
            pad_len = model_worker_batch.input_ids.shape[0] - hidden_states.shape[0]
            pad_shape = (pad_len,) + hidden_states.shape[1:]
            pad_values = jnp.zeros(pad_shape, dtype=hidden_states.dtype)
            model_worker_batch.spec_info.hidden_states = jnp.concatenate(
                [model_worker_batch.spec_info.hidden_states, pad_values], axis=0
            )

        return model_worker_batch, logits_metadata

    def prepare_for_decode(self, schedule_batch: ScheduleBatch):
        new_allocate_lens = np.array(schedule_batch.seq_lens + self.ALLOC_LEN_PER_DECODE)
        self.allocate_lens = np.array(self.allocate_lens)
        bs = schedule_batch.batch_size()
        assert (
            self.allocate_lens.shape[0] == bs
        ), f" {self.allocate_lens.shape[0]=} but batch_size is {bs} "
        page_size = schedule_batch.token_to_kv_pool_allocator.page_size

        if page_size == 1:
            num_needed_tokens = (new_allocate_lens - self.allocate_lens).sum().item()
            out_cache_loc = alloc_token_slots(schedule_batch.tree_cache, num_needed_tokens)
        else:
            last_loc = get_last_loc(
                schedule_batch.req_to_token_pool.req_to_token,
                schedule_batch.req_pool_indices,
                self.allocate_lens,
            )
            extend_num_tokens = sum(new_allocate_lens - self.allocate_lens).item()
            out_cache_loc = alloc_paged_token_slots_extend(
                schedule_batch.tree_cache,
                self.allocate_lens,
                new_allocate_lens,
                last_loc,
                extend_num_tokens,
            )
        assign_req_to_token_pool(
            schedule_batch.req_pool_indices,
            schedule_batch.req_to_token_pool,
            self.allocate_lens,
            new_allocate_lens,
            out_cache_loc,
        )

        self.allocate_lens = new_allocate_lens

        schedule_batch.seq_lens_sum = np.sum(schedule_batch.seq_lens).item()
        schedule_batch.out_cache_loc = out_cache_loc

    def prepare_for_draft_decode(
        self, model_worker_batch: ModelWorkerBatch, topk: int, num_steps: int
    ):
        self.capture_hidden_mode = CaptureHiddenMode.LAST
        self.num_tokens_per_batch = topk
        self.num_tokens_for_logprob_per_batch = topk
        model_worker_batch.return_hidden_states = False
        # bs = model_worker_batch.seq_lens.shape[0]
        # we don't need process outcache_loc because we don't need this in attention backend
        # out_cache_loc = np.empty((bs * topk * num_steps), dtype=np.int32)
        # model_worker_batch.out_cache_loc = out_cache_loc
        model_worker_batch.seq_lens_sum = np.sum(model_worker_batch.seq_lens)
        model_worker_batch.return_hidden_states = False
        model_worker_batch.spec_info.positions = np.repeat(model_worker_batch.seq_lens, topk)

    @classmethod
    def create_idle_input(
        cls,
        hidden_size: int,
        dtype: np.dtype,
        topk: int,
        capture_hidden_mode: CaptureHiddenMode,
    ):
        return cls(
            verified_id=np.empty((0,), dtype=np.int32),
            hidden_states=np.empty((0, hidden_size), dtype=dtype),
            topk_p=np.empty((0, topk), dtype=np.float32),
            topk_index=np.empty((0, topk), dtype=np.int32),
            capture_hidden_mode=capture_hidden_mode,
            accept_length=np.empty((0,), dtype=np.int32),
            accept_length_cpu=np.empty((0,), dtype=np.int32),
        )

    @DeprecationWarning
    def prepare_extend_after_decode(
        self,
        batch: ScheduleBatch,
    ):
        if batch.forward_mode.is_idle():
            return

        batch.input_ids = self.verified_id
        accept_length_cpu_arr = batch.spec_info.accept_length_cpu
        if accept_length_cpu_arr is None:
            accept_length_cpu_host = np.asarray([], dtype=np.int32)
        else:
            accept_length_cpu_host = accept_length_cpu_arr
        batch.extend_lens = (accept_length_cpu_host + 1).tolist()
        batch.extend_num_tokens = sum(batch.extend_lens)
        batch.seq_lens = batch.spec_info.seq_lens_for_draft_extend
        batch.seq_lens_sum = batch.seq_lens.sum().item()
        batch.req_pool_indices = batch.spec_info.req_pool_indices_for_draft_extend
        batch.return_logprob = False
        batch.return_hidden_states = False

        self.capture_hidden_mode = CaptureHiddenMode.LAST
        self.accept_length = self.accept_length + 1
        self.positions = np.empty_like(batch.input_ids, dtype=np.int32)
        self.verified_id = np.empty_like(self.accept_length, dtype=np.int32)

        self.positions, self.verified_id = create_extend_after_decode_spec_info(
            batch.input_ids,
            batch.seq_lens,
            self.accept_length,
            self.positions,
            self.verified_id,
        )

        self.accept_length_cpu = np.asarray(accept_length_cpu_host, dtype=np.int32)

    def filter_batch(self, new_indices: np.ndarray, has_been_filtered: bool = True):

        if has_been_filtered:
            # in eagle_utils.py:verify, we have already filtered the batch by `unfinished_index`
            # therefore, we don't need to filter the batch again in scheduler
            if len(new_indices) != len(self.topk_p):
                logger.warning(
                    "length of new_indices: %d != length of topk_p: %d, this should not happen",
                    len(new_indices),
                    len(self.topk_p),
                )
            self.topk_p = self.topk_p[: len(new_indices)]
            self.topk_index = self.topk_index[: len(new_indices)]
            self.hidden_states = self.hidden_states[: len(new_indices)]
            self.verified_id = self.verified_id[: len(new_indices)]
        else:
            # in some cases(e.g draft_extend), we have not filtered the batch by `unfinished_index`
            self.topk_p = self.topk_p[new_indices]
            self.topk_index = self.topk_index[new_indices]
            self.hidden_states = self.hidden_states[new_indices]
            self.verified_id = self.verified_id[new_indices]

    def merge_batch(self, spec_info: EagleDraftInput):
        # FIXME(pc) need support overlap here
        if self.hidden_states is None:
            self.hidden_states = spec_info.hidden_states
            self.verified_id = spec_info.verified_id
            self.topk_p = spec_info.topk_p
            self.topk_index = spec_info.topk_index
            return
        if spec_info.hidden_states is None:
            return
        # FIXME(pc) this operate should be put on cpu
        self.hidden_states = np.concatenate([self.hidden_states, spec_info.hidden_states], axis=0)
        self.verified_id = np.concatenate([self.verified_id, spec_info.verified_id], axis=0)
        self.topk_p = np.concatenate([self.topk_p, spec_info.topk_p])
        self.topk_index = np.concatenate([self.topk_index, spec_info.topk_index])
        self.allocate_lens = np.concatenate([self.allocate_lens, spec_info.allocate_lens])


@dataclass
class EagleVerifyOutput:
    # Draft input batch
    draft_input: EagleDraftInput
    # Logit outputs from target worker
    logits_output: LogitsProcessorOutput
    # Accepted token ids including the bonus token
    verified_id: np.ndarray
    # Accepted token length per sequence in a batch in CPU.
    accept_length_per_req_cpu: list[int]
    # Accepted indices from logits_output.next_token_logits
    accepted_indices: np.ndarray


@register_pytree_node_class
@dataclass
class EagleVerifyInput:
    # container type for pytree
    draft_token: jax.Array
    custom_mask: jax.Array
    positions: jax.Array
    retrive_index: jax.Array
    retrive_next_token: jax.Array
    retrive_next_sibling: jax.Array
    retrive_cum_len: jax.Array
    seq_lens_cpu: np.ndarray
    # common type for pytree
    spec_steps: int
    topk: int
    draft_token_num: int
    seq_lens_sum: int
    capture_hidden_mode: CaptureHiddenMode
    # grammar: BaseGrammarObject = None

    def tree_flatten(self):
        seq_lens_sum_arr = _as_int32_array(self.seq_lens_sum, fallback=0)

        children = (
            self.draft_token,
            self.custom_mask,
            self.positions,
            self.retrive_index,
            self.retrive_next_token,
            self.retrive_next_sibling,
            self.retrive_cum_len,
            self.seq_lens_cpu,
            seq_lens_sum_arr,
        )

        aux_data = {
            "spec_steps": self.spec_steps,
            "topk": self.topk,
            "draft_token_num": self.draft_token_num,
            "capture_hidden_mode": self.capture_hidden_mode,
        }
        return (children, aux_data)

    @classmethod
    def tree_unflatten(cls, aux_data, children):
        obj = cls.__new__(cls)
        obj.spec_steps = aux_data["spec_steps"]
        obj.topk = aux_data["topk"]
        obj.draft_token_num = aux_data["draft_token_num"]
        obj.capture_hidden_mode = aux_data["capture_hidden_mode"]

        obj.draft_token = children[0]
        obj.custom_mask = children[1]
        obj.positions = children[2]
        obj.retrive_index = children[3]
        obj.retrive_next_token = children[4]
        obj.retrive_next_sibling = children[5]
        obj.retrive_cum_len = children[6]
        obj.seq_lens_cpu = children[7]
        obj.seq_lens_sum = children[8]

        return obj

    def prepare_for_verify(
        self, model_worker_batch: ModelWorkerBatch, page_size: int, target_worker: ModelWorker
    ):
        if model_worker_batch.forward_mode.is_idle():
            return

        # TODO: keep draft_token on TPU
        # bs = len(model_worker_batch.req_pool_indices)
        model_worker_batch.input_ids = self.draft_token
        model_worker_batch.positions = self.positions
        # bs = batch.batch_size()
        # prefix_lens = model_worker_batch.seq_lens
        # seq_lens_with_draft_token = model_worker_batch.seq_lens + self.draft_token_num
        # extend_lens = jnp.array([self.draft_token_num] * bs)
        model_worker_batch.return_hidden_states = False
        model_worker_batch.forward_mode = ForwardMode.TARGET_VERIFY
        model_worker_batch.spec_info = self
        model_worker_batch.capture_hidden_mode = CaptureHiddenMode.FULL

        # FIXME(pc) maybe not need here
        model_worker_batch.extend_seq_lens = self.draft_token
        # assert model_worker_batch.capture_hidden_mode == spec_info.capture_hidden_mode

    def sample(
        self,
        model_worker_batch: ModelWorkerBatch,
        logits_output: LogitsProcessorOutput,
        # token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        # page_size: int,
        rng: nnx.Rngs,
        mesh: Mesh,
        # vocab_mask: jax.Array | None = None,  # For grammar
    ) -> jax.Array:
        """
        Verify and find accepted tokens based on logits output and batch
        (which contains spec decoding information).

        WARNING: This API in-place modifies the states of logits_output

        This API updates values inside logits_output based on the accepted
        tokens. I.e., logits_output.next_token_logits only contains
        accepted token logits.
        """
        if model_worker_batch.forward_mode.is_idle():
            return EagleVerifyOutput(
                draft_input=EagleDraftInput.create_idle_input(
                    hidden_size=model_worker_batch.model_config.hidden_size,
                    dtype=model_worker_batch.model_config.dtype,
                    topk=self.topk,
                    capture_hidden_mode=CaptureHiddenMode.LAST,
                ),
                logits_output=logits_output,
                verified_id=jnp.empty(0, dtype=jnp.int32),
                accept_length_per_req_cpu=[],
                accepted_indices=jnp.full(
                    (0, self.spec_steps + 1),
                    -1,
                    dtype=jnp.int32,
                ),
            )

        bs = self.retrive_index.shape[0]
        candidates = self.draft_token.reshape(bs, self.draft_token_num)
        sampling_info = model_worker_batch.sampling_info

        predict_shape = list(logits_output.next_token_logits.shape)[:-1]
        predict_shape[-1] += 1
        predict = jnp.zeros(predict_shape, dtype=jnp.int32)

        accept_index = jnp.full((bs, self.spec_steps + 1), -1, dtype=jnp.int32)
        accept_length = jnp.zeros((bs,), dtype=jnp.int32)

        if bs != len(sampling_info):
            sampling_info = copy.deepcopy(sampling_info)
            # NOTE: retrive_index are the indices of the requests that are kept.
            # FIXME(pc)
            sampling_info.filter_batch(np.arange(len(sampling_info)))

        # TODO: support custom sampler, apply the custom logit processors if registered in the sampling info.
        # if sampling_info.has_custom_logit_processor:
        #     pass
        # TODO: Apply penalty
        # if sampling_info.penalizer_orchestrator.is_required:
        #     pass
        # TODO: Apply grammar mask
        # if vocab_mask is not None:
        #     pass

        # Sample tokens. Force greedy sampling on AMD
        is_all_greedy = sampling_info.is_all_greedy

        if is_all_greedy:
            target_predict = jnp.argmax(logits_output.next_token_logits, axis=-1).flatten()
            # # for compatibility, 0.6.3 need to use use_mesh. set_mesh is not have __entry__ attribute.
            # # on jax >=0.7.1, we need to use set_mesh.
            try:
                ctx = jax.sharding.use_mesh(mesh)
            except AttributeError:
                try:
                    ctx = jax.set_mesh(mesh)
                except AttributeError:
                    ctx = mesh
            with ctx:
                print(f"{predict.shape=}")
                print(f"{accept_index.shape=}")
                print(f"{accept_length.shape=}")
                print(f"{candidates.shape=}")
                print(
                    f"{self.retrive_next_token.shape=} {np.min(self.retrive_next_token)=} {np.max(self.retrive_next_token)=}"
                )
                print(
                    f"{self.retrive_index.shape=} {np.min(self.retrive_index)=} {np.max(self.retrive_index)=}"
                )
                print(
                    f"{self.retrive_next_sibling.shape=} {np.min(self.retrive_next_sibling)=} {np.max(self.retrive_next_sibling)=}"
                )
                print(f"{target_predict.shape=}")

                accept_index, accept_length, predict = verify_tree_greedy(
                    predicts=predict,
                    accept_index=accept_index,
                    accept_token_num=accept_length,
                    candidates=candidates,
                    retrive_index=self.retrive_index,
                    retrive_next_token=self.retrive_next_token,
                    retrive_next_sibling=self.retrive_next_sibling,
                    target_predict=target_predict,
                )
        else:
            # apply temperature and get target probs
            expanded_temperature = jnp.repeat(
                sampling_info.temperatures, self.draft_token_num
            )  # (bs * draft_token_num, 1)
            expanded_temperature = jnp.expand_dims(expanded_temperature, axis=-1)
            target_probs = jax.nn.softmax(
                logits_output.next_token_logits / expanded_temperature, axis=-1
            )  # (bs * draft_token_num, vocab_size)
            target_probs = top_k_renorm_prob(
                target_probs, jnp.repeat(sampling_info.top_ks, self.draft_token_num)
            )

            if not jnp.all(sampling_info.top_ps == 1.0):
                target_probs = top_p_renorm_prob(
                    target_probs, jnp.repeat(sampling_info.top_ps, self.draft_token_num)
                )

            # TODO: optimize top_k and top_p by avoiding sort
            rngs = jax.random.split(rng.params(), 3)

            draft_probs = jnp.zeros(target_probs.shape, dtype=jnp.float32)

            # coins for rejection sampling
            coins = jax.random.uniform(rngs[1], candidates.shape, dtype=jnp.float32)
            # coins for final sampling
            coins_for_final_sampling = jax.random.uniform(rngs[2], (bs,), dtype=jnp.float32)
            accept_index, accept_length, predict = tree_speculative_sampling_target_only(
                predicts=predict,
                accept_index=accept_index,
                accept_token_num=accept_length,
                candidates=candidates,
                retrive_index=self.retrive_index,
                retrive_next_token=self.retrive_next_token,
                retrive_next_sibling=self.retrive_next_sibling,
                uniform_samples=coins,
                uniform_samples_for_final_sampling=coins_for_final_sampling,
                target_probs=target_probs,
                draft_probs=draft_probs,
                threshold_single=global_server_args_dict["speculative_accept_threshold_single"],
                threshold_acc=global_server_args_dict["speculative_accept_threshold_acc"],
                deterministic=True,
            )
        if SIMULATE_ACC_LEN:
            # Do simulation
            _, rng = jax.random.split(rng.params())
            accept_index, accept_length, predict = _generate_simulated_accept_index(
                accept_index=accept_index,
                predict=predict,
                accept_length=accept_length,
                simulate_acc_len=SIMULATE_ACC_LEN,
                bs=bs,
                spec_steps=self.spec_steps,
                rng=rng,
            )

        accept_length = accept_length + 1
        print(f"1     {accept_index.shape=}")
        accept_index = accept_index.reshape(-1)
        print(f"2     {accept_index.shape=}")

        return predict, accept_length, accept_index


def _generate_simulated_accept_index(
    accept_index: jax.Array,
    predict,
    accept_length,
    simulate_acc_len,
    bs,
    spec_steps,
    rng: nnx.Rngs,
):
    simulate_acc_len_float = float(simulate_acc_len)
    if SIMULATE_ACC_METHOD == "multinomial":
        # here data is on cpu
        simulated_values = numpy.random.normal(
            loc=simulate_acc_len_float,
            scale=1.0,
            size=(1,),
        )
        # clamp simulated values to be between 1 and self.spec_steps
        simulated_values = jnp.clip(simulated_values, min=1.0, max=spec_steps + 1)
        simulate_acc_len = int(simulated_values.round().item())
    elif SIMULATE_ACC_METHOD == "match-expected":
        # multinomial sampling does not match the expected length
        # we keep it for the sake of compatibility of existing tests
        # but it's better to use "match-expected" for the cases that need to
        # match the expected length, One caveat is that this will only sample
        # either round down or round up of the expected length
        simulate_acc_len_float = max(1.0, min(spec_steps + 1, simulate_acc_len_float))
        lower = int(simulate_acc_len_float // 1)
        upper = lower + 1 if lower < spec_steps + 1 else lower
        if lower == upper:
            simulate_acc_len = lower
        else:
            weight_upper = simulate_acc_len_float - lower
            weight_lower = 1.0 - weight_upper
            # here, data is on cpu
            probs = jnp.array([weight_lower, weight_upper])
            sampled_index = jax.random.categorical(rng, jnp.log(probs))
            simulate_acc_len = lower if sampled_index == 0 else upper
    else:
        raise ValueError(f"Invalid simulate_acc_method: {SIMULATE_ACC_METHOD}")

    accept_indx_first_col = accept_index[:, 0].reshape(-1, 1)
    sim_accept_index = jnp.full((bs, spec_steps + 1), -1, dtype=jnp.int32)
    sim_accept_index = sim_accept_index.at[:, :simulate_acc_len].set(
        accept_indx_first_col + jnp.arange(simulate_acc_len)
    )
    accept_length = accept_length.at[:].set(simulate_acc_len - 1)
    predict = predict.at[:].set(100)  # some legit token id
    return sim_accept_index, accept_length, predict


def get_src_tgt_cache_loc(
    seq_lens: np.ndarray,
    out_cache_loc: np.ndarray,
    accept_index: np.ndarray,
    accept_length: np.ndarray,
    draft_token_num: int,
    page_size: int,
):
    src_cache_loc = out_cache_loc[accept_index]
    extended_len = seq_lens + draft_token_num
    keep_len = np.minimum(
        (seq_lens + accept_length + 1 + page_size - 1) // page_size * page_size,
        extended_len,
    )
    to_free_num_slots = extended_len - keep_len
    return src_cache_loc, to_free_num_slots


def create_accept_length_filter(
    accept_length: np.ndarray,
    unfinished_index_device: np.ndarray,
    seq_lens: np.ndarray,
):
    accept_length_filter = jnp.zeros_like(accept_length)
    accept_length_filter[unfinished_index_device] = accept_length[unfinished_index_device] + 1
    seq_lens.add_(accept_length + 1)
    return accept_length_filter


def assign_req_to_token_pool(
    req_pool_indices,
    req_to_token_pool,
    start_offsets,
    end_offsets,
    out_cache_loc,
):
    bs = start_offsets.shape[0]
    out_cache_lens = end_offsets - start_offsets
    out_cache_loc_start_positions = np.concatenate(
        [np.array([0], dtype=np.int32), np.cumsum(out_cache_lens)]
    )[0:-1]
    all_cache_loc_len = out_cache_loc.shape[0]
    allocate_len = 0
    for i in range(bs):
        out_cache_loc_start = out_cache_loc_start_positions[i]
        req_to_token_pool.write(
            (req_pool_indices[i], slice(start_offsets[i], end_offsets[i])),
            out_cache_loc[out_cache_loc_start : out_cache_loc_start + out_cache_lens[i]],
        )
        allocate_len += out_cache_lens[i]
    assert (
        allocate_len == all_cache_loc_len
    ), f"not all allocate cache loc is assigned to req_token_pool, it's may lead to mem leak, assigned {allocate_len}, allocate {all_cache_loc_len}"
