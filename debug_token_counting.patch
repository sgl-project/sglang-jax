"""Debug patch for token counting issue.

Add this to scheduler_metrics_mixin.py to get detailed token counting debug info.
"""

# In python/sgl_jax/srt/managers/scheduler_metrics_mixin.py
# Add debug logging to log_decode_stats:

def log_decode_stats(self: Scheduler, running_batch: ScheduleBatch = None):
    batch = running_batch or self.running_batch

    gap_latency = time.perf_counter() - self.last_decode_stats_tic
    self.last_decode_stats_tic = time.perf_counter()
    self.last_gen_throughput = self.num_generated_tokens / gap_latency
    self.num_generated_tokens = 0
    num_running_reqs = len(batch.reqs)

    # === DEBUG: Add detailed token info ===
    if self.is_hybrid:
        (
            full_num_used,
            swa_num_used,
            full_token_usage,
            swa_token_usage,
            full_available_size,
            swa_available_size,
            full_evictable_size,
            swa_evictable_size,
        ) = self._get_swa_token_info()
        num_used = max(full_num_used, swa_num_used)
        token_usage = max(full_token_usage, swa_token_usage)
        token_msg = (
            f"#full token: {full_num_used}, "
            f"full token usage: {full_token_usage:.2f}, "
            f"#swa token: {swa_num_used}, "
            f"swa token usage: {swa_token_usage:.2f}, "
        )
        # DEBUG
        logger.info(f"[DEBUG] SWA Token Details:")
        logger.info(f"  full_max_total: {self.full_tokens_per_layer}")
        logger.info(f"  swa_max_total: {self.swa_tokens_per_layer}")
        logger.info(f"  full_available: {full_available_size}, full_evictable: {full_evictable_size}")
        logger.info(f"  swa_available: {swa_available_size}, swa_evictable: {swa_evictable_size}")
    else:
        num_used, token_usage, available_size, evictable_size = self._get_token_info()
        token_msg = f"#token: {num_used}, " f"token usage: {token_usage:.2f}, "

        # === DEBUG: Log detailed breakdown ===
        logger.info(f"[DEBUG] Token Count Breakdown:")
        logger.info(f"  max_total_num_tokens: {self.max_total_num_tokens}")
        logger.info(f"  available_size: {available_size}")
        logger.info(f"  evictable_size: {evictable_size}")
        logger.info(f"  num_used (calculated): {num_used}")
        logger.info(f"  page_size: {self.token_to_kv_pool_allocator.page_size}")

        # Check allocator details
        allocator = self.token_to_kv_pool_allocator
        if hasattr(allocator, 'free_pages'):
            logger.info(f"  free_pages count: {len(allocator.free_pages)}")
            logger.info(f"  release_pages count: {len(allocator.release_pages)}")
            logger.info(f"  total_free_pages: {len(allocator.free_pages) + len(allocator.release_pages)}")
            if hasattr(allocator, 'num_pages'):
                logger.info(f"  num_pages: {allocator.num_pages}")

        # Calculate expected per request
        if num_running_reqs > 0:
            avg_per_req = num_used / num_running_reqs
            logger.info(f"  avg_tokens_per_request: {avg_per_req:.0f}")

            # Check actual sequence lengths
            seq_lens = [req.seqlen for req in batch.reqs]
            total_seq_lens = sum(seq_lens)
            logger.info(f"  total_seq_lens (sum of req.seqlen): {total_seq_lens}")
            logger.info(f"  seq_lens: {seq_lens}")

            # With page alignment
            if hasattr(allocator, 'page_size') and allocator.page_size > 1:
                aligned_total = sum(
                    ((seq_len + allocator.page_size - 1) // allocator.page_size) * allocator.page_size
                    for seq_len in seq_lens
                )
                logger.info(f"  page_aligned_total: {aligned_total}")
                logger.info(f"  difference (num_used - aligned_total): {num_used - aligned_total}")

    if RECORD_STEP_TIME:
        self.step_time_dict[num_running_reqs].append(
            gap_latency / self.server_args.decode_log_interval
        )

    msg = f"Decode batch. #running-req: {num_running_reqs}, {token_msg}"

    if running_batch.spec_algorithm is not None and not running_batch.spec_algorithm.is_none():
        accept_ratio = self.accept_token / self.draft_token
        accept_len = self.accept_token / self.spec_num_forward_ct
        self.accept_token = 0
        self.draft_token = 0
        self.spec_num_forward_ct = 0
        msg += f"accept-len {accept_len:.2f}, accept-ratio {accept_ratio:.2f}, "

    msg += (
        f"gen throughput (token/s): {self.last_gen_throughput:.2f}, "
        f"#queue-req: {len(self.waiting_queue)}, "
    )

    if batch.cache_miss_count > 0:
        msg += f"#cache_miss: {batch.cache_miss_count}"

    logger.info(msg)
