"""
ç®€åŒ–çš„profilingï¼šé€šè¿‡è°ƒæ•´blockå‚æ•°æ¥åˆ†æDMA vs è®¡ç®—æ¯”ä¾‹
"""

import time

import jax
import numpy as np
from utils import create_prefill_uniform_data

from sgl_jax.srt.layers.attention.flash_attn_kernel.flash_attention import (
    ragged_paged_attention,
)


def benchmark_separated():
    """é€šè¿‡ä¸åŒçš„blocké…ç½®æ¥é—´æ¥åˆ†æDMA vs è®¡ç®—æ¯”ä¾‹"""

    # ä½¿ç”¨ç›¸åŒçš„æ•°æ®é…ç½®
    batch_size, seq_len, num_heads, head_dim = 2, 2048, 8, 128
    page_size = 128
    max_kv_cache_tokens_num = 120000

    q, k, v, _, page_indices, cu_q_lens, cu_kv_lens, num_seqs, seq_lens, _ = (
        create_prefill_uniform_data(
            batch_size,
            seq_len,
            seq_len,
            max_kv_cache_tokens_num,
            num_heads,
            head_dim,
            page_size=page_size,
        )
    )

    def benchmark_config(name, kv_pages, q_block):
        print(f"\n=== {name} ===")
        print(f"KV pages per block: {kv_pages}, Queries per block: {q_block}")

        # ä¸ºæ¯ä¸ªé…ç½®åˆ›å»ºå•ç‹¬çš„jitå‡½æ•°
        @jax.jit
        def flash_attention_config():
            return ragged_paged_attention(
                q,
                k,
                v,
                page_indices,
                cu_q_lens,
                cu_kv_lens,
                num_seqs,
                seq_lens,
                sm_scale=head_dim**-0.5,
                num_kv_pages_per_block=kv_pages,
                num_queries_per_block=q_block,
            )

        # é¢„çƒ­
        result = flash_attention_config()
        jax.block_until_ready(result)

        # æµ‹è¯•
        times = []
        for i in range(5):
            start = time.perf_counter()
            result = flash_attention_config()
            jax.block_until_ready(result)
            times.append(time.perf_counter() - start)

        avg_time = np.mean(times) * 1000
        print(f"Average time: {avg_time:.3f} ms")
        return avg_time

    # æµ‹è¯•ä¸åŒé…ç½®æ¥åˆ†æç“¶é¢ˆï¼ˆè°ƒæ•´ä¸º32M VMEMå‹å¥½çš„é…ç½®ï¼‰
    results = {}

    # 1. æå°å—ï¼šæµ‹è¯•DMAå¼€é”€æé™
    results["Tiny_blocks"] = benchmark_config(
        "æå°å— (æœ€å¤§DMAå¼€é”€)", kv_pages=1, q_block=8
    )

    # 2. DMAå¯†é›†å‹ï¼šå°å—ï¼Œé¢‘ç¹ä¼ è¾“
    results["DMA_intensive"] = benchmark_config(
        "DMAå¯†é›†å‹ (å°å—é¢‘ç¹ä¼ è¾“)", kv_pages=2, q_block=16
    )

    # 3. ä¸­ç­‰é…ç½®ï¼šå¹³è¡¡
    results["Balanced"] = benchmark_config("å¹³è¡¡é…ç½®", kv_pages=4, q_block=16)

    # 4. è®¡ç®—å¯†é›†å‹ï¼šç¨å¤§å—ï¼Œå‡å°‘ä¼ è¾“ï¼ˆè°ƒå°é¿å…OOMï¼‰
    results["Compute_intensive"] = benchmark_config(
        "è®¡ç®—å¯†é›†å‹ (å¤§å—å°‘ä¼ è¾“)", kv_pages=8, q_block=24
    )

    print(f"\n{'='*50}")
    print("åˆ†æç»“æœ:")
    print(f"{'='*50}")

    dma_intensive = results["DMA_intensive"]
    compute_intensive = results["Compute_intensive"]
    tiny_blocks = results["Tiny_blocks"]
    balanced = results["Balanced"]

    print(f"DMAå¯†é›†å‹:    {dma_intensive:.3f} ms")
    print(f"è®¡ç®—å¯†é›†å‹:   {compute_intensive:.3f} ms")
    print(f"å¹³è¡¡é…ç½®:     {balanced:.3f} ms")
    print(f"æå°å—:       {tiny_blocks:.3f} ms")

    # åˆ†æç“¶é¢ˆ
    if tiny_blocks > dma_intensive * 1.5:
        print("\nğŸ” DMAå¼€é”€å¾ˆå¤§ï¼šæå°å—æ¯”DMAå¯†é›†å‹æ…¢å¾ˆå¤š")
        bottleneck = "DMA setup overhead"
    elif compute_intensive < dma_intensive * 0.8:
        print("\nğŸ” è®¡ç®—æ˜¯ç“¶é¢ˆï¼šå¤§å—é…ç½®æ˜¾è‘—æ›´å¿«")
        bottleneck = "Compute bound"
    else:
        print("\nğŸ” ç›¸å¯¹å¹³è¡¡ï¼šä¸åŒé…ç½®æ€§èƒ½ç›¸è¿‘")
        bottleneck = "Balanced"

    # DMA vs è®¡ç®—æ¯”ä¾‹ä¼°ç®—
    dma_overhead_ratio = (dma_intensive - compute_intensive) / dma_intensive * 100
    print(f"\nDMAå¼€é”€å æ¯”ä¼°ç®—: ~{dma_overhead_ratio:.1f}%")
    print(f"ä¸»è¦ç“¶é¢ˆ: {bottleneck}")

    return results, bottleneck


if __name__ == "__main__":
    benchmark_separated()
